\chapter*{Contrastive Learning of Musical Representations}
\section*{Summary}

This thesis contributes a novel learning framework for artificial neural networks on music.
It makes one of the first attempts to artificially model musical knowledge, without any human teaching it \textit{anything} about what music is to us, using self-supervised learning. The neural network has to figure out `how music works' all by itself - despite that its only observations are waveforms\TFfootnote{An audio waveform is the measure of an air molecule's displacement over time.} of music.

This thesis focuses on a learning paradigm within the area of `unsupervised learning' that is called `self-supervised learning'.
% It generally defines a `simple' auxiliary task: by witholding part of an observation, the task of the network becomes to correctly predict that observation.
It generally defines a simple auxiliary task. Simply put, we `pretend' there is a part of the observation we do not know, and we ask the network to predict the missing observation.

This work presents the learning task as follows: given any library of music audio, 96 different music fragments\TFfootnote{Depending on available computing power, this number can be smaller or larger.} are randomly picked per training iteration. Each fragment is randomly augmented using a rich, carefully designed sequence of musical transformations, e.g., the music key is shifted up or down, or the room acoustics are changed. This results in 96 pairs of correlated, but different `versions' of music fragments. Subsequently, each individual, augmented music fragment is `encoded' to a `latent representation'. Finally, the network simply learns to perform the following task: find the pairs of augmented music fragments that belong together.\TFfootnote{The rationale behind this contrastive learning strategy is \emph{predictive coding}, a theory that the human brain encodes causal structures and predicts future events at different levels of abstraction} This novel approach to learning from music is called: CLMR.

After it has finished learning in this `pre-training' phase, its expressivity is evaluated using a challenging, human task: music classification. This is also called the `fine-tuning' phase. We use two music corpora that have multiple semantic tags associated for each music fragment. These tags describe many characteristics of music, e.g., genre, instrumentation and dynamics. We limit the capacity of the network in this final learning phase using a single, linear "outer" layer. Simply put: a linear classifier is generally very limited in solving complex tasks like semantic labeling of very high-dimensional audio signals of music. But, given an expressive "inner" network that has learned important, musical knowledge from the `pre-training' phase, a linear "outer" layer could solve the task very well.

This thesis shows that a \textit{linear} classifier fine-tuned on representations from a pre-trained CLMR model achieves higher music classification performance compared to an `end-to-end' supervised neural network. This work also carefully studies the contribution of each musical transformation: stronger augmentations generally lead to better representations. CLMR also requires far less human-annotated labels to perform music classification: only 259 songs are needed to achieve competitive performance. Finally, CLMR is also able to generalise and learn important musical knowledge from entirely different music corpora. Because we think interpretability of neural networks is essential to progress in this research field, we perform a thorough investigation on the nature of what the network has learned. This shows that the network learns a frequency filter response similar to the Mel-scale - a scale that better reflects how humans hear pitch.

CLMR requires no preprocessing of music and learns without ground truth, which enables simple and straightforward pre-training on datasets of unprecedented scale, requiring no human labeling. Its simplicity, together with encouraging results obtained with a single linear layer optimised for a challenging downstream task, are exciting developments towards unsupervised learning on music audio waveforms.


To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this thesis \TFfootnote{\url{https://github.com/Spijkervet/CLMR}}.




